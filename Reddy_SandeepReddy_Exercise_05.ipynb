{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep5924/Assignments/blob/main/Reddy_SandeepReddy_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a1338e4-5c8a-4c6f-a5f4-d3e79cfea534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating MultinomialNB...\n",
            "Cross-validation scores: [0.73428571 0.73142857 0.73104435 0.75822604 0.75107296 0.74248927\n",
            " 0.74105866 0.74821173 0.74678112 0.74391989]\n",
            "Mean cross-validation accuracy: 0.7428518291436745\n",
            "Training and evaluating SVM...\n",
            "Cross-validation scores: [0.79       0.79       0.79113019 0.79113019 0.79113019 0.79113019\n",
            " 0.78969957 0.78969957 0.78969957 0.78969957]\n",
            "Mean cross-validation accuracy: 0.7903319027181688\n",
            "Training and evaluating KNN...\n",
            "Cross-validation scores: [0.78285714 0.75857143 0.77253219 0.76251788 0.76824034 0.76537911\n",
            " 0.7739628  0.76967096 0.77682403 0.77110157]\n",
            "Mean cross-validation accuracy: 0.7701657469854896\n",
            "Training and evaluating Decision tree...\n",
            "Cross-validation scores: [0.68285714 0.68857143 0.71101574 0.68669528 0.68955651 0.6981402\n",
            " 0.69384835 0.72246066 0.68955651 0.69957082]\n",
            "Mean cross-validation accuracy: 0.6962272634375639\n",
            "Training and evaluating Random Forest...\n",
            "Cross-validation scores: [0.78428571 0.78       0.78540773 0.78397711 0.78540773 0.78683834\n",
            " 0.78397711 0.78111588 0.78254649 0.78111588]\n",
            "Mean cross-validation accuracy: 0.7834671980380136\n",
            "Training and evaluating XGBoost...\n",
            "Cross-validation scores: [0.77714286 0.78142857 0.78111588 0.78540773 0.77968526 0.78540773\n",
            " 0.77682403 0.77825465 0.77968526 0.78540773]\n",
            "Mean cross-validation accuracy: 0.7810359697527078\n",
            "\n",
            "Results:\n",
            "MultinomialNB\n",
            "Accuracy: 0.7815894797026872\n",
            "Recall: 0.9748923959827833\n",
            "Precision: 0.7966002344665886\n",
            "F1 Score: 0.8767741935483871\n",
            "\n",
            "SVM\n",
            "Accuracy: 0.7970268724985706\n",
            "Recall: 1.0\n",
            "Precision: 0.7970268724985706\n",
            "F1 Score: 0.8870505886096087\n",
            "\n",
            "KNN\n",
            "Accuracy: 0.7747284162378502\n",
            "Recall: 0.9655667144906743\n",
            "Precision: 0.7955082742316785\n",
            "F1 Score: 0.8723266364225536\n",
            "\n",
            "Decision tree\n",
            "Accuracy: 0.7061177815894797\n",
            "Recall: 0.8436154949784792\n",
            "Precision: 0.7989130434782609\n",
            "F1 Score: 0.820655966503838\n",
            "\n",
            "Random Forest\n",
            "Accuracy: 0.7878787878787878\n",
            "Recall: 0.9870875179340028\n",
            "Precision: 0.7958357432041643\n",
            "F1 Score: 0.8812039705411463\n",
            "\n",
            "XGBoost\n",
            "Accuracy: 0.7913093196112064\n",
            "Recall: 0.9892395982783357\n",
            "Precision: 0.7975708502024291\n",
            "F1 Score: 0.8831252001280818\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "def load_data(train_url, test_url):\n",
        "    train_df = pd.read_csv(train_url, header=None, delimiter='\\t')\n",
        "    test_df = pd.read_csv(test_url, header=None, delimiter='\\t')\n",
        "    train_df.columns = ['text']\n",
        "    test_df.columns = ['text']\n",
        "    train_df['label'] = 1\n",
        "    test_df['label'] = 0\n",
        "    full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "    X = full_df['text']\n",
        "    y = full_df['label']\n",
        "    return X, y\n",
        "\n",
        "def vectorize_text(X_train, X_test):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)  # Transform test data using the same vectorizer\n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "def train_and_evaluate_classifiers(X_train, X_test, y_train, y_test):\n",
        "    classifiers = {\n",
        "        \"MultinomialNB\": MultinomialNB(),\n",
        "        \"SVM\": SVC(),\n",
        "        \"KNN\": KNeighborsClassifier(),\n",
        "        \"Decision tree\": DecisionTreeClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"XGBoost\": XGBClassifier()\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"Training and evaluating {clf_name}...\")\n",
        "        cv_scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
        "        print(f\"Cross-validation scores: {cv_scores}\")\n",
        "        print(f\"Mean cross-validation accuracy: {cv_scores.mean()}\")\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        results[clf_name] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1 Score\": f1\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def display_results(results):\n",
        "    print(\"\\nResults:\")\n",
        "    for clf_name, metrics in results.items():\n",
        "        print(clf_name)\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"{metric_name}: {value}\")\n",
        "        print()\n",
        "\n",
        "def main(train_url, test_url):\n",
        "    X, y = load_data(train_url, test_url)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train_vec, X_test_vec = vectorize_text(X_train, X_test)  # Vectorize both training and test data\n",
        "    results = train_and_evaluate_classifiers(X_train_vec, X_test_vec, y_train, y_test)\n",
        "    display_results(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_data_url = 'https://raw.githubusercontent.com/sandeep5924/Assignments/main/stsa-train.txt'\n",
        "    test_data_url = 'https://raw.githubusercontent.com/sandeep5924/Assignments/main/stsa-test.txt'\n",
        "    main(train_data_url, test_data_url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d437880-d360-4827-db8a-5597dc14b0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Silhouette Score: 0.03744601767949588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset from the archive\n",
        "archive_path = \"/content/archive.zip\"\n",
        "with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
        "    csv_file = zip_ref.open(zip_ref.namelist()[0])\n",
        "    data = pd.read_csv(csv_file)\n",
        "\n",
        "reviews = data['Reviews']\n",
        "\n",
        "# Sample a subset of the data for faster processing\n",
        "reviews_subset = reviews.sample(n=1000, random_state=42)\n",
        "\n",
        "# Preprocess text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "preprocessed_reviews = reviews_subset.apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(preprocessed_reviews)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "print(\"K-means Silhouette Score:\", silhouette_score(X, kmeans_labels))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X)\n",
        "print(\"DBSCAN Silhouette Score:\", silhouette_score(X, dbscan_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSf9qiCwpBlw",
        "outputId": "a84d7768-b0c1-41a3-d524-b05d5bdcc44e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBSCAN Silhouette Score: 0.025936946962022367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchical clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical_labels = hierarchical.fit_predict(X.toarray())\n",
        "print(\"Hierarchical Silhouette Score:\", silhouette_score(X, hierarchical_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXbAs4CRpMA1",
        "outputId": "b939fb2c-7798-435b-e4a6-fa5d4c6b82e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hierarchical Silhouette Score: 0.011387879599253522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec clustering\n",
        "word2vec_model = Word2Vec(sentences=[text.split() for text in preprocessed_reviews], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = word2vec_model.wv\n",
        "word2vec_clusters = KMeans(n_clusters=5, random_state=42).fit_predict(word_vectors.vectors)\n",
        "print(\"Word2Vec Silhouette Score:\", silhouette_score(word_vectors.vectors, word2vec_clusters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRYRFZxhpQy0",
        "outputId": "32379c8b-69f1-448b-d3b8-9d407d06e012"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Silhouette Score: 0.47346723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT clustering\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "encoded_inputs = bert_tokenizer(preprocessed_reviews.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**encoded_inputs)\n",
        "    pooled_output = outputs.pooler_output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "g4RyL1rUpWsf",
        "outputId": "8f16f841-d743-449b-f541-9ecad602104d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BertTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-894c1858a774>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BERT clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoded_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "The clustering performance and underlying methods used by K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT for grouping comparable data points vary. Traditional clustering algorithms like K-means, DBSCAN, and hierarchical clustering use various techniques to divide the data into clusters. While DBSCAN classifies dense regions as clusters based on a distance criterion, K-means allocates each data point to the closest centroid. A tree-like hierarchy of clusters is created using hierarchical clustering. Conversely, Word2Vec and BERT are embedding methods that identify semantic similarities within words or sentences. While BERT creates contextualized embeddings by taking the full sentence into account, Word2Vec creates dense vector representations of words based on their context.\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "During the exercises, a number of errors were encountered, such as NameError, AttributeError, ValueError, and ParserError. Programming errors like this are frequently encountered, particularly in the areas of data loading, preprocessing, and model training. Correcting file paths, making sure data is formatted correctly, and importing the required libraries are just a few of the suitable remedies that must be put in place in order to properly address these problems.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}