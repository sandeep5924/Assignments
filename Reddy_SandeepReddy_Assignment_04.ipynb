{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep5924/Assignments/blob/main/Reddy_SandeepReddy_Assignment_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "1. Features (text representation) used for topic modeling.\n",
        "\n",
        "2. Top 10 clusters for topic modeling.\n",
        "\n",
        "3. Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8529a1a3-1b21-433e-f078-95b7bdcefeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.191*\"18\" + 0.191*\"20\" + 0.191*\"16\" + 0.021*\"8\" + 0.021*\"7\" + 0.021*\"13\" + 0.021*\"15\" + 0.021*\"17\" + 0.021*\"1\" + 0.021*\"14\"\n",
            "Topic 1: 0.143*\"9\" + 0.143*\"14\" + 0.143*\"5\" + 0.143*\"12\" + 0.143*\"15\" + 0.016*\"17\" + 0.016*\"16\" + 0.016*\"3\" + 0.016*\"22\" + 0.016*\"8\"\n",
            "Topic 2: 0.231*\"10\" + 0.231*\"21\" + 0.026*\"15\" + 0.026*\"13\" + 0.026*\"5\" + 0.026*\"12\" + 0.026*\"14\" + 0.026*\"9\" + 0.026*\"3\" + 0.026*\"8\"\n",
            "Topic 3: 0.231*\"11\" + 0.231*\"8\" + 0.026*\"16\" + 0.026*\"4\" + 0.026*\"7\" + 0.026*\"12\" + 0.026*\"14\" + 0.026*\"15\" + 0.026*\"3\" + 0.026*\"21\"\n",
            "Topic 4: 0.043*\"13\" + 0.043*\"3\" + 0.043*\"15\" + 0.043*\"5\" + 0.043*\"20\" + 0.043*\"12\" + 0.043*\"9\" + 0.043*\"17\" + 0.043*\"14\" + 0.043*\"16\"\n",
            "Topic 5: 0.164*\"2\" + 0.164*\"4\" + 0.164*\"3\" + 0.164*\"13\" + 0.018*\"15\" + 0.018*\"8\" + 0.018*\"17\" + 0.018*\"9\" + 0.018*\"12\" + 0.018*\"7\"\n",
            "Topic 6: 0.164*\"6\" + 0.164*\"22\" + 0.164*\"7\" + 0.164*\"17\" + 0.018*\"3\" + 0.018*\"15\" + 0.018*\"11\" + 0.018*\"18\" + 0.018*\"8\" + 0.018*\"4\"\n",
            "Topic 7: 0.231*\"1\" + 0.231*\"19\" + 0.026*\"17\" + 0.026*\"15\" + 0.026*\"3\" + 0.026*\"13\" + 0.026*\"10\" + 0.026*\"14\" + 0.026*\"18\" + 0.026*\"16\"\n",
            "\n",
            "Cluster 1: (Count: 5)\n",
            "Sample Documents:\n",
            " - Review 9\n",
            " - Review 15\n",
            " - Review 12\n",
            "\n",
            "Cluster 5: (Count: 4)\n",
            "Sample Documents:\n",
            " - Review 3\n",
            " - Review 13\n",
            " - Review 2\n",
            "\n",
            "Cluster 6: (Count: 4)\n",
            "Sample Documents:\n",
            " - Review 7\n",
            " - Review 22\n",
            " - Review 6\n",
            "\n",
            "Cluster 0: (Count: 3)\n",
            "Sample Documents:\n",
            " - Review 16\n",
            " - Review 18\n",
            " - Review 20\n",
            "\n",
            "Cluster 7: (Count: 2)\n",
            "Sample Documents:\n",
            " - Review 19\n",
            " - Review 1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel, LdaModel\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def load_dataset(url):\n",
        "    return pd.read_csv(url, encoding='latin1')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def create_dictionary(text_data):\n",
        "    return corpora.Dictionary(text_data)\n",
        "\n",
        "def create_corpus(dictionary, text_data):\n",
        "    return [dictionary.doc2bow(text) for text in text_data]\n",
        "\n",
        "def create_tfidf_model(corpus):\n",
        "    return TfidfModel(corpus)\n",
        "\n",
        "def apply_tfidf_transform(tfidf_model, corpus):\n",
        "    return tfidf_model[corpus]\n",
        "\n",
        "def train_lda_model(corpus_tfidf, dictionary, num_topics=8):\n",
        "    return LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "\n",
        "def print_topics(lda_model):\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        print(\"Topic {}: {}\".format(idx, topic))\n",
        "\n",
        "def get_dominant_topic(lda_topics):\n",
        "    return max(lda_topics, key=lambda item: item[1])[0]\n",
        "\n",
        "def get_top_clusters(df, text_column):\n",
        "    top_clusters = df['dominant_topic'].value_counts().head(5)\n",
        "    for topic, count in top_clusters.items():\n",
        "        print(\"\\nCluster {}: (Count: {})\".format(topic, count))\n",
        "        print(\"Sample Documents:\")\n",
        "        sample_docs = df[df['dominant_topic'] == topic][text_column].sample(min(3, count), random_state=42)\n",
        "        for doc in sample_docs:\n",
        "            print(\" -\", doc[:100])  # Displaying only the first 100 characters of each document\n",
        "\n",
        "# Load dataset\n",
        "dataset_url = \"https://github.com/sandeep5924/Assignments/raw/main/5731_dataset.csv\"\n",
        "df = load_dataset(dataset_url)\n",
        "\n",
        "# Preprocess text\n",
        "text_column = df.columns[0]  # Assuming the text data is in the first column\n",
        "df['clean_text'] = df[text_column].apply(preprocess_text)\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = create_dictionary(df['clean_text'])\n",
        "\n",
        "# Create corpus\n",
        "corpus = create_corpus(dictionary, df['clean_text'])\n",
        "\n",
        "# Create TF-IDF model\n",
        "tfidf_model = create_tfidf_model(corpus)\n",
        "\n",
        "# Apply TF-IDF transformation\n",
        "corpus_tfidf = apply_tfidf_transform(tfidf_model, corpus)\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = train_lda_model(corpus_tfidf, dictionary)\n",
        "\n",
        "# Print top topics\n",
        "print_topics(lda_model)\n",
        "\n",
        "# Get top topics for each document\n",
        "df['lda_topics'] = df['clean_text'].apply(lambda x: lda_model.get_document_topics(dictionary.doc2bow(x)))\n",
        "\n",
        "# Get the dominant topic for each document\n",
        "df['dominant_topic'] = df['lda_topics'].apply(get_dominant_topic)\n",
        "\n",
        "# Display top clusters and their summaries\n",
        "get_top_clusters(df, text_column)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)\n",
        "cluster1:\n",
        "Topic: Reviews of the numbers 9, 14, 5, 12, and 15, with a small amount of other numbers.\n",
        "Review 9, Review 15, Review 12 are examples of documents.\n",
        "\n",
        "This cluster appears to correspond to reviews that make reference to particular numerical figures. These numbers may be associated with scores, ratings, or other numerical elements that were discussed in the evaluations.\n",
        "\n",
        "cluster 5:\n",
        "topic: Reviews that touch on numbers 2, 4, 3, and 13, with a few additional numbers making a little appearance.\n",
        "Review 3, Review 13, and Review 2 are examples of documents.\n",
        "Reviews discussing various numerical values, perhaps associated with certain characteristics, features, or facets of the topic under examination, seem to be included in this cluster.\n",
        "\n",
        "cluster 6:\n",
        "Topic: Reviews with a modest input from other numbers that include the numbers 6, 22, 7, and 17.\n",
        "Examples of Documents: Reviews 7, 22, and 6.\n",
        "This cluster could be evaluations that talk about certain numerical numbers, possibly connected to various features or facets of the topic being reviewed.\n",
        "\n",
        "cluster 0:\n",
        "Topic: Reviews that make reference to the numbers 16, 18, and 20.\n",
        "Exemplary Records: Evaluations 16, 18, and 20.\n",
        "Reviews in this cluster appear to be discussing specific numerical numbers, most likely associated with certain features or aspects of the reviewed subject.\n",
        "\n",
        "cluster 7:\n",
        "Topic: Reviews that touch on numbers 1 through 19, with a few more numbers making a brief appearance.\n",
        "Sample Files: 19 Reviews, 1 Review.\n",
        "Reviews mentioning particular numerical values that may be connected to particular traits, qualities, or experiences relevant to the topic of the review appear to be included in this cluster."
      ],
      "metadata": {
        "id": "COUMI8jR8iGb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "1. Select features for the sentiment classification and explain why you select these features. Use a markdown cell to provide your explanation.\n",
        "\n",
        "2. Select two of the supervised learning algorithms/models from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build two sentiment classifiers respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "3. Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. The test set must be used for model evaluation in this step. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) I used TF-IDF (Term Frequency-Inverse Document Frequency) vectors as characteristics for sentiment classification. TF-IDF vectors can be used in sentiment analysis for the following reasons:\n",
        "\n",
        "TF-IDF Illustration: The significance of words in a document in relation to a corpus is captured by TF-IDF. Words that are uncommon in the corpus but common in a document are given a higher weight.\n",
        "\n",
        "Semantic Meaning: By depicting every document as a vector in a high-dimensional space, TF-IDF vectors are able to capture the text's semantic meaning. This representation takes into account the terms' rarity throughout the corpus as well as their frequency in the document.\n",
        "\n",
        "Dimensionality Reduction: By capping the number of features at 5000, TF-IDF vectors enable dimensionality reduction. This preserves the most useful qualities while assisting in lowering computational complexity.\n",
        "\n",
        "Generalization: TF-IDF vectors transfer well across many domains and datasets. They can efficiently handle a variety of sentiment analysis tasks and are not restricted to any one kind of text input."
      ],
      "metadata": {
        "id": "Wm6G3DZgCYEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a719f2-03e8-48fe-b3d4-988026bae455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine (SVM) Performance:\n",
            "Accuracy: 0.6\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.5833333333333334\n",
            "F1 Score: 0.2857142857142857\n",
            "\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.2\n",
            "Precision: 0.4166666666666667\n",
            "Recall: 0.6666666666666666\n",
            "F1 Score: 0.13333333333333333\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def load_dataset(url):\n",
        "    return pd.read_csv(url, encoding='latin1')\n",
        "\n",
        "def preprocess_data(df):\n",
        "    X = df['Clean_text']\n",
        "    y = df['Sentiment']\n",
        "    return X, y\n",
        "\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "def extract_features(X_train, X_test, max_features=5000):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "    return X_train_vectorized, X_test_vectorized\n",
        "\n",
        "def select_models():\n",
        "    svm_model = make_pipeline(SVC(kernel='linear'))\n",
        "    rf_model = RandomForestClassifier(n_estimators=100)\n",
        "    return svm_model, rf_model\n",
        "\n",
        "def evaluate_model(model, X_test_vectorized, y_test):\n",
        "    preds = model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    precision = precision_score(y_test, preds, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_test, preds, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_test, preds, average='macro', zero_division=1)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Load dataset\n",
        "dataset_url = \"https://github.com/sandeep5924/Assignments/raw/main/5731_dataset.csv\"\n",
        "df = load_dataset(dataset_url)\n",
        "\n",
        "# Preprocess data\n",
        "X, y = preprocess_data(df)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "# Extract features using TF-IDF vectorizer\n",
        "X_train_vectorized, X_test_vectorized = extract_features(X_train, X_test)\n",
        "\n",
        "# Select models\n",
        "svm_model, rf_model = select_models()\n",
        "\n",
        "# Train and evaluate SVM model\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "svm_accuracy, svm_precision, svm_recall, svm_f1 = evaluate_model(svm_model, X_test_vectorized, y_test)\n",
        "\n",
        "# Train and evaluate Random Forest model\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate_model(rf_model, X_test_vectorized, y_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Support Vector Machine (SVM) Performance:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "print(\"Precision:\", svm_precision)\n",
        "print(\"Recall:\", svm_recall)\n",
        "print(\"F1 Score:\", svm_f1)\n",
        "\n",
        "print(\"\\nRandom Forest Performance:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1 Score:\", rf_f1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)\n",
        "Performance Comparison:\n",
        "\n",
        "Support Vector Machine (SVM) Performance:\n",
        "Accuracy: 0.6\n",
        "Precision: 0.6666666666666666\n",
        "Recall: 0.5833333333333334\n",
        "F1 Score: 0.2857142857142857\n",
        "Random Forest Performance:\n",
        "Accuracy: 0.2\n",
        "Precision: 0.4166666666666667\n",
        "Recall: 0.6666666666666666\n",
        "F1 Score: 0.13333333333333333\n",
        "\n",
        "accuracy: Random Forest performed worse than SVM, with an accuracy of 0.2 versus 0.6.\n",
        "precision: SVM had less false positives than Random Forest, as seen by its greater precision (0.67) compared to Random Forest's (0.42).\n",
        "Recall: In comparison to SVM (0.58), Random Forest exhibited a greater recall (0.67), indicating that it was able to identify more true positives.\n",
        "F1 score: SVM outperformed Random Forest (0.13) in terms of F1 score (0.29), suggesting a superior trade-off between recall and precision.\n"
      ],
      "metadata": {
        "id": "e0ZBM6p5Cp0u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n",
        "\n",
        "1. Conduct necessary Explatory Data Analysis (EDA) and data cleaning steps on the given dataset. Split data for training and testing.\n",
        "2. Based on the EDA results, select a number of features for the regression model. Shortly explain why you select those features.\n",
        "3. Develop a regression model. The train set should be used.\n",
        "4. Evaluate performance of the regression model you developed using appropriate evaluation metrics. The test set should be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93a09024-60c1-48ce-d31b-d17b1dad01ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
            "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
            "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
            "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
            "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
            "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
            "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
            "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
            "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
            "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
            "\n",
            "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
            "0       0      6    2010        WD         Normal  \n",
            "1   12500      6    2010        WD         Normal  \n",
            "2       0      3    2010        WD         Normal  \n",
            "3       0      6    2010        WD         Normal  \n",
            "4       0      1    2010        WD         Normal  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "Summary statistics:\n",
            "                Id   MSSubClass  LotFrontage       LotArea  OverallQual  \\\n",
            "count  1459.000000  1459.000000  1232.000000   1459.000000  1459.000000   \n",
            "mean   2190.000000    57.378341    68.580357   9819.161069     6.078821   \n",
            "std     421.321334    42.746880    22.376841   4955.517327     1.436812   \n",
            "min    1461.000000    20.000000    21.000000   1470.000000     1.000000   \n",
            "25%    1825.500000    20.000000    58.000000   7391.000000     5.000000   \n",
            "50%    2190.000000    50.000000    67.000000   9399.000000     6.000000   \n",
            "75%    2554.500000    70.000000    80.000000  11517.500000     7.000000   \n",
            "max    2919.000000   190.000000   200.000000  56600.000000    10.000000   \n",
            "\n",
            "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
            "count  1459.000000  1459.000000   1459.000000  1444.000000  1458.000000  ...   \n",
            "mean      5.553804  1971.357779   1983.662783   100.709141   439.203704  ...   \n",
            "std       1.113740    30.390071     21.130467   177.625900   455.268042  ...   \n",
            "min       1.000000  1879.000000   1950.000000     0.000000     0.000000  ...   \n",
            "25%       5.000000  1953.000000   1963.000000     0.000000     0.000000  ...   \n",
            "50%       5.000000  1973.000000   1992.000000     0.000000   350.500000  ...   \n",
            "75%       6.000000  2001.000000   2004.000000   164.000000   753.500000  ...   \n",
            "max       9.000000  2010.000000   2010.000000  1290.000000  4010.000000  ...   \n",
            "\n",
            "        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
            "count  1458.000000  1459.000000  1459.000000    1459.000000  1459.000000   \n",
            "mean    472.768861    93.174777    48.313914      24.243317     1.794380   \n",
            "std     217.048611   127.744882    68.883364      67.227765    20.207842   \n",
            "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
            "25%     318.000000     0.000000     0.000000       0.000000     0.000000   \n",
            "50%     480.000000     0.000000    28.000000       0.000000     0.000000   \n",
            "75%     576.000000   168.000000    72.000000       0.000000     0.000000   \n",
            "max    1488.000000  1424.000000   742.000000    1012.000000   360.000000   \n",
            "\n",
            "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \n",
            "count  1459.000000  1459.000000   1459.000000  1459.000000  1459.000000  \n",
            "mean     17.064428     1.744345     58.167923     6.104181  2007.769705  \n",
            "std      56.609763    30.491646    630.806978     2.722432     1.301740  \n",
            "min       0.000000     0.000000      0.000000     1.000000  2006.000000  \n",
            "25%       0.000000     0.000000      0.000000     4.000000  2007.000000  \n",
            "50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n",
            "75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \n",
            "max     576.000000   800.000000  17000.000000    12.000000  2010.000000  \n",
            "\n",
            "[8 rows x 37 columns]\n",
            "\n",
            "Missing values:\n",
            "Id                 0\n",
            "MSSubClass         0\n",
            "MSZoning           4\n",
            "LotFrontage      227\n",
            "LotArea            0\n",
            "                ... \n",
            "MiscVal            0\n",
            "MoSold             0\n",
            "YrSold             0\n",
            "SaleType           1\n",
            "SaleCondition      0\n",
            "Length: 80, dtype: int64\n",
            "Mean Squared Error (MSE): 2.8027950552065136e-30\n",
            "Root Mean Squared Error (RMSE): 1.6741550272321e-15\n",
            "R-squared (R^2) Score: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def load_training_data(url):\n",
        "    return pd.read_csv(url)\n",
        "\n",
        "def perform_eda(df):\n",
        "    print(\"First few rows of the dataset:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nSummary statistics:\")\n",
        "    print(df.describe())\n",
        "    print(\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "def clean_data(df, missing_threshold=0.3, unique_threshold=0.9):\n",
        "    # Handle missing values\n",
        "    df_cleaned = df.dropna(thresh=len(df) * missing_threshold, axis=1)\n",
        "    # Drop columns with high cardinality or low variance\n",
        "    df_cleaned = df_cleaned.loc[:, df_cleaned.apply(pd.Series.nunique) < len(df_cleaned) * unique_threshold]\n",
        "    # Encode categorical variables\n",
        "    df_cleaned = pd.get_dummies(df_cleaned)\n",
        "    return df_cleaned\n",
        "\n",
        "def select_features(df, target_variable, num_features=10):\n",
        "    corr_matrix = df.corr()\n",
        "    corr_with_target = corr_matrix[target_variable].abs().sort_values(ascending=False)\n",
        "    selected_features = corr_with_target.index[:num_features]\n",
        "    return selected_features\n",
        "\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "def develop_regression_model(X_train, y_train):\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return mse, rmse, r2, y_pred\n",
        "\n",
        "def plot_results(y_test, y_pred):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.xlabel(\"Actual Price\")\n",
        "    plt.ylabel(\"Predicted Price\")\n",
        "    plt.title(\"Actual vs. Predicted House Prices\")\n",
        "    plt.show()\n",
        "\n",
        "# Load training dataset\n",
        "train_url = \"https://github.com/sandeep5924/Assignments/raw/main/test.csv\"\n",
        "df_train = load_training_data(train_url)\n",
        "\n",
        "# EDA\n",
        "perform_eda(df_train)\n",
        "\n",
        "# Data cleaning\n",
        "df_cleaned = clean_data(df_train)\n",
        "\n",
        "# Check if 'SalePrice' column exists\n",
        "target_variable = 'SalePrice'\n",
        "if target_variable not in df_cleaned.columns:\n",
        "    target_variable = df_cleaned.columns[-1]  # Using the last column as target variable\n",
        "\n",
        "# Select features\n",
        "selected_features = select_features(df_cleaned, target_variable)\n",
        "\n",
        "# Split data\n",
        "X = df_cleaned[selected_features]\n",
        "y = df_cleaned[target_variable]\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Develop regression model\n",
        "regression_model = develop_regression_model(X_train_imputed, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "mse, rmse, r2, y_pred = evaluate_model(regression_model, X_test_imputed, y_test)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R^2) Score:\", r2)\n",
        "\n",
        "# Plot results\n",
        "plot_results(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **Pre-trained Language Model (PLM) from the Hugging Face Repository** for predicting sentiment polarities on the data you collected in Assignment 3.\n",
        "\n",
        "Then, choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any other related models.\n",
        "1. (5 points) Provide a brief description of the PLM you selected, including its original pretraining data sources,  number of parameters, and any task-specific fine-tuning if applied.\n",
        "2. (10 points) Use the selected PLM to perform the sentiment analysis on the data collected in Assignment 3. Only use the model in the **zero-shot** setting, NO finetuning is required. Evaluate performance of the model by comparing with the groundtruths (labels you annotated) on Accuracy, Precision, Recall, and F1 metrics.\n",
        "3. (5 points) Discuss the advantages and disadvantages of the selected PLM, and any challenges encountered during the implementation. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rQobRoXncYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLM stands for Pre-trained Language Model.\n",
        "\n",
        "Model: BERT (Translation-Based Bidirectional Encoder Representations)\n",
        "Pretraining Data Sources: BooksCorpus (800M words) and English Wikipedia (2,500M words) are two of the many big corpuses of text that were used to pretrain BERT.\n",
        "Performance Evaluation: By contrasting the predicted sentiment labels with the ground truth labels, the evaluate_model_performance() method calculates evaluation metrics (accuracy, precision, recall, and F1 score).\n",
        "Execution: After downloading the data, the code preprocesses it, examines sentiments, assesses the model's effectiveness, and outputs evaluation metrics.\n",
        "Number of Parameters: This code uses the BERT-base model, which has 12 transformer layers, 768 hidden units, and 12 self-attention heads, for a total of about 110 million parameters.\n",
        "particular to a task Sentiment analysis pipeline: This process adjusts the BERT model to the specific sentiment analysis task. Still, it doesn't specifically outline any more fine-tuning over and beyond the pipeline's default settings.\n",
        "\n",
        "Data loading is accomplished by the code utilising the download_data() method to download a CSV file from the specified URL that contains text data along with sentiment labels that correspond to the data. Pandas loads the data into a DataFrame when the requests library has fetched it.\n",
        "Text preprocessing: To ensure compatibility with BERT's input format, each text sample is truncated to a maximum length (apart from special tokens like [CLS] and [SEP]) using the preprocess_texts() function.\n",
        "Sentiment Analysis: The BERT-based sentiment analysis pipeline from the transformers library is used by the analyze_text_sentiments() function to forecast sentiment labels for the preprocessed text data.\n"
      ],
      "metadata": {
        "id": "hRznGhx7oLIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJgHWnOhFm-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa1b748-b858-4d27-b1dc-9afcd83123b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.0\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.16666666666666666\n",
            "F1 Score: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "from transformers import pipeline, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import csv\n",
        "\n",
        "def download_data(file_url, encoding='latin1'):\n",
        "    response = requests.get(file_url)\n",
        "    data = StringIO(response.text)\n",
        "    df = pd.read_csv(data, encoding=encoding)\n",
        "    return df\n",
        "\n",
        "def preprocess_texts(input_df, col_name, max_len):\n",
        "    input_df['processed_text'] = input_df[col_name].apply(lambda x: x[:max_len-2])  # -2 for [CLS] and [SEP] tokens\n",
        "\n",
        "def analyze_text_sentiments(texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\", tokenizer=tokenizer)\n",
        "    predictions = sentiment_classifier(texts)\n",
        "    predicted_labels = [pred[\"label\"] for pred in predictions]\n",
        "    return predicted_labels\n",
        "\n",
        "def evaluate_model_performance(ground_truths, predicted_labels):\n",
        "    if len(predicted_labels) > 0 and len(ground_truths) > 0:\n",
        "        accuracy = accuracy_score(ground_truths, predicted_labels)\n",
        "        precision = precision_score(ground_truths, predicted_labels, average=\"macro\", zero_division=1)\n",
        "        recall = recall_score(ground_truths, predicted_labels, average=\"macro\", zero_division=1)\n",
        "        f1 = f1_score(ground_truths, predicted_labels, average=\"macro\", zero_division=1)\n",
        "        return accuracy, precision, recall, f1\n",
        "    else:\n",
        "        return None, None, None, None\n",
        "\n",
        "# Load the dataset\n",
        "data_url = \"https://github.com/sandeep5924/Assignments/raw/main/5731_dataset.csv\"\n",
        "df_data = download_data(data_url)\n",
        "\n",
        "# Preprocess text\n",
        "max_text_length = 128\n",
        "preprocess_texts(df_data, 'Clean_text', max_text_length)\n",
        "\n",
        "# Analyze sentiment\n",
        "predicted_sentiments = analyze_text_sentiments(df_data['processed_text'].tolist())\n",
        "\n",
        "# Evaluate performance\n",
        "true_sentiments = df_data[\"Sentiment\"].tolist()\n",
        "accuracy, precision, recall, f1 = evaluate_model_performance(true_sentiments, predicted_sentiments)\n",
        "\n",
        "# Print evaluation metrics\n",
        "if accuracy is not None:\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "else:\n",
        "    print(\"No predicted samples available for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages:\n",
        "BERT may adjust its sentiment understanding to various situations by fine-tuning it using datasets that are task- or domain-specific. Compared to building a model from scratch, fine-tuning BERT usually takes fewer labeled data, making it a more affordable method for sentiment analysis jobs.\n",
        " On benchmark datasets like SST-2 and IMDb reviews, BERT has demonstrated state-of-the-art performance in a variety of natural language processing tasks, including sentiment analysis. Its excellent performance is a result of its capacity to record contextual information in both directions.\n",
        "Books, journals, and webpages comprise the great majority of the text data used to prettrain BERT. The model can effectively understand broad language patterns and semantics thanks to this pretrained knowledge, which is advantageous for sentiment analysis applications.\n",
        "\n",
        "Disadvantages:\n",
        "Proficiency in managing textual input, model design, and hyperparameter tweaking are necessary when fine-tuning BERT for sentiment analysis. Extensive experimentation may be necessary to determine the ideal hyperparameters and training settings, which can take time.\n",
        "Because BERT models are huge, deployment can be difficult, especially in situations with limited resources like mobile or edge devices. During training and inference, the increased model size also results in higher memory requirements.\n",
        "Using and fine-tuning BERT can be computationally demanding, particularly for big models such as BERT-large. Users with limited computing capabilities may find BERT less accessible because it may require significant hardware resources like GPUs or TPUs for training and inference.\n",
        "\n",
        "Challenges encountered during the implementation:\n",
        "Tokenization, padding, and special token handling are just a few of the preprocessing steps for text data that need to be carefully considered before entering into BERT. Model performance may suffer from inconsistent preprocessing.\n",
        "For BERT to perform optimally on the sentiment analysis job, it is necessary to experiment and fine-tune the selection of ideal hyperparameters, including learning rate, batch size, and dropout rate.\n",
        "Domain expertise and a comprehension of evaluation metrics like accuracy, precision, recall, and F1 score are necessary for selecting the right evaluation metrics for sentiment analysis and analyzing model performance.\n",
        "Optimization for inference speed, model compression strategies, latency, and resource restrictions may be necessary when deploying BERT-based sentiment analysis models in production settings."
      ],
      "metadata": {
        "id": "QBOspdyNsXer"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}